<!doctype html>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-146776294-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-146776294-1');
</script>

<meta name="viewport" content="width=device-width, initial-scale=1.0">
<html lang="English">
<head>
<meta charset="utf-8">
<meta name="generator" content="pandoc">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>ElectionModel</title>

<!-- Yahoo! CDN combo URL for selected Pure.css modules -->
<link rel="stylesheet" href="https://yui.yahooapis.com/combo?pure/0.6.0/base-min.css&pure/0.6.0/grids-responsive-min.css&pure/0.6.0/menus-min.css&pure/0.6.0/tables-min.css">
<!-- MathJax -->
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- Vega and Vega Embed -->
<script src="https://cdn.jsdelivr.net/npm/vega@5"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-lite@5"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-embed@6"></script>

<!-- Extra styles -->
<style>
  @viewport {
      width: device-width;
      zoom: 1.0;
  }
  body {margin:0 0 0}.pure-g{padding:0 1em}.pure-menu-link:focus{background-color:#d3d3d3}nav{margin:0 0 1em;padding:0 0 1em;border-bottom:1px solid #ccc}footer{margin:5em 0 1em}pre{white-space:pre-wrap;margin-left:3em}code{font-size:89%;color:#191919}.author{margin-bottom:0;padding-bottom:0}.headnote,.published,.license{font-size:89%;margin-bottom:.75em}@media screen and (max-width:35.5em){thead{display:none}tr,th,td{display:block}td{border-top:0}tr td:first-child{border-top:1px solid #ddd;font-weight:700}}
  .br-header {
      font-family: proxima-nova, sans-serif;
      letter-spacing: 2px;
      font-weight: 500;
      font-size: 14pt;
      color: white;
      margin-top: 0em;
      margin-left: 0em;
      margin-bottom: 1em;
      margin-right: 0.1em;
      display: flex;
      flex-direction: row;
      flex-wrap: nowrap;
      justify-content: space-between;
      background-color: black;
      padding: 5px;
      padding-top: 20px;
      padding-bottom: 20px;
  }
  .br-header-logo-wrapper
  {
      width: 140px;
      background-color: white;
      height: 58px;
      margin-left: 10px;
  }
  .br-header-logo
  {
      width: 140px;
      object-fit: contain;
  }
  .br-header-item
  {
      display: flex;
      flex-basis: auto;
  }
  .br-header-br {
      margin-left: 10px;
  }
  .br-header-br a:link, .br-header-br a:visited, .brheader a:hover, .br-header-br a:active
  {
      color: white;
      text-decoration: none;
  }
  .br-home-button-c {
      flex-direction: column;
      justify-content: center;
      margin-right: 10px;
  }
  .br-home-button-b {
      font-size: 10pt;
      border-style: solid;
      border-width: 2px;
      border-color: white;
      border-radius: 25px;
      padding: 5px 20px;
      color: white;
      text-align: center;
  }
  .br-home-button-b a:hover {
      background-color: white;
  }
  .br-readmore {
      font-style: italic;
  }
  .brTable {
      font-size: 180t
  }
  .brTableTitle {
      text-align: center;
      font-size:  14pt
  }
  .footnotes {
      font-size: 10pt;
  }
  @media screen and (min-width: 600px) {
     .content-wrapper {
	 margin-left: 150px;
	 margin-right: 150px
     }
  }
  @media screen and (max-width: 600px) {
      .content-wrapper {
	  margin-left: 10px;
	  margin-right: 10px
      }
  }
  figure {
      margin-left: 0px;
      margin-right: 0px;
  }
  a, a:link, a:visited, a:hover, a:active {
      color: hsl(175, 43%, 42%);
      text-decoration: none;
  }
  h1 {
      font-family: Garamond;
      font-weight: 400;
      color: hsla(0,0%,10%,0.9);
      line-height: 1.2em;
      font-size: 32px;
      letter-spacing: 0px
  }
  h2 {
      font-family: proxima-nova, sans-serif;
      font-weight: 600;
      color: hsla(0,0%,10%,0.9);
      line-height: 1.2em;
      letter-spacing: 0.05em;
      font-size: 25px;
      text-transform: uppercase;
  }
  .published {
      color: lightslategrey;
      font-family: Garamond;
      font-size: 12pt
  }
  .updated {
      color: lightslategrey;
      font-family: Garamond;
      font-size: 12pt
  }
  body {
      font-family: Garamond;
      font-weight: 400;
      color: hsla(0,0%,10%,0.7);
      font-size: 18px;
      letter-spacing: 0px;
      line-height: 1.6em;
  }
  table {
      font-size: 10pt;
      margin: 0;
      padding: 0;
      table-layout: auto;
      width: 100%;
  }
  table tr {
      background-color: #f8f8f8;
      border: 0.5px solid #ddd;
      padding: .35em;
  }
  table th,
  table td {
      padding: .125 em;
      border: 1px solid black;
  }
  @media screen and (max-width: 600px) {
      table thead {
	  border: none;
	  clip: rect(0 0 0 0);
	  height: 1px;
	  margin: -1px;
	  overflow: hidden;
	  padding: 0;
	  position: absolute;
	  width: 1px;
      }

      table tr {
	  border-bottom: 3px solid #ddd;
	  display: block;
      }

      table td {
	  border-bottom: 1px solid #ddd;
	  display: block;
	  text-align: right;
      }

      table td::before {
	  content: attr(data-label);
	  float: left;
      }
  }
  table-old {
      border-collapse: collapse;
      font-size: 10pt;
      margin-left: auto;
      margin-right: auto;
      margin-bottom: 24px;
      border-spacing: 0;
      border-bottom: 2px solid black;
      border-top: 2px solid black;
  }
  figure {
      border: 1px #cccccc solid;
      padding: 4px;
      margin: auto
  }
  figcaption {
      font-style: italic;
      font-size: 12pt;
      text-align: center;
  }
</style>

<script src="" type="text/javascript"></script>
</head>
<body>
  <div class="br-header">
    <a href="https://www.blueripplepolitics.org">
      <div class="br-header-item br-header-logo-wrapper">
	<img class="br-header-logo" src="https://blueripple.github.io/logo/full.png" alt="BLUE RIPPLE POLITICS"/>
      </div>
    </a>
    <div class="br-header-item br-home-button-c">
      <a href="https://www.blueripplepolitics.org">
	<div class="br-home-button-b">HOME</div>
      </a>
    </div>
  </div>
<section id="page-content">
<div class="content-wrapper pure-g">
<div class="pure-u-1 pure-u-sm-1 pure-u-md-1 pure-u-lg-1 pure-u-xl-1">

<!-- page content begins here -->

<div class="published">September 23, 2021 (updated: August 24, 2022)</div>
<section id="blue-ripples-demographic-election-model" class="level1">
<h1>Blue Ripple’s Demographic Election Model</h1>
<ol type="1">
<li>Introduction</li>
<li>Definitions and Notation</li>
<li>Model Dependencies</li>
<li>Model Paramters and Structure</li>
</ol>
<section id="introduction" class="level2">
<h2>Introduction</h2>
<p>We want to build a reasonable but simple demographic model of voter
turnout and party preference which we can use to predict the outcome of
an election in a geography with known demographics, like a congressional
or state-legisltive district. And, since this is a redistricting year,
we want something that we can apply to newly drawn districts as well as
existing ones.</p>
<p>By “predict,” we don’t mean in the sense of actually trying to
handicap the outcome. Instead we intend to estimate the outcome based
only on geographic and demographic variables. So we ignore things like
current national sentiment toward either party, whether or not the
candidate is an incumbent, etc. Our goal is to isolate something like an
intrinsic partisan lean, though we readily acknowledge that this can
shift over time as voter turnout and preferences shift. We’re trying to
strike a balance between an estimate which just assembles voting history
for all the precincts in a district, e.g., <a
href="https://davesredistricting.org/maps#aboutus">Dave’s
Redistricting</a> and a full-blown prediction model, using demographics,
poll averages, and economic-indicators, e.g., the <a
href="https://projects.economist.com/us-2020-forecast/president">Economist
Model</a>.</p>
<p>We don’t expect our model to be more accurate than a historical or
full-blown predictive model. Instead, we hope to spot places where the
demographic expectation and historical results don’t line up, and to do
so well in advance of any polling or in places where polls don’t exist,
like state-legislative races.</p>
<p>We could work bottom-up, using voting precincts as building
blocks:</p>
<ol type="1">
<li>Get the geographic boundary and election results for each precinct,
e.g., from <a href="https://openprecincts.org">openprecincts</a>.</li>
<li>Build a demographic profile of the precinct using overlaps<a
href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a> of the precinct and census “blocks,”
“block groups” or “tracts”<a href="#fn2" class="footnote-ref"
id="fnref2" role="doc-noteref"><sup>2</sup></a>.</li>
<li>Use the total number of votes and votes cast for the Democratic
candidate in each precinct to <em>infer</em> a demographic and
geographic model of turnout and voter preference in the entire set of
precincts.</li>
<li>Apply that model to the demographics of a district to generate a
rough estimate of the likely election result.</li>
</ol>
<p>For a given district (or set of districts), what precincts do we
include in the model? In order to keep things simple we want a model
that covers multiple districts. We could model using every precinct in
the country or at least the state. Using every precinct in the country
is hard: some of that data is unavailable and there are a lot of
precincts (about <a
href="https://arxiv.org/ftp/arxiv/papers/1410/1410.8868.pdf">175,000</a>
of them)! Using data only from the state risks being too influenced by
the local election history.</p>
<p>So instead, we work top-down from national-level data:</p>
<ol type="1">
<li>Use a national survey of voter turnout, e.g., the Current Population
Survey Voter Registration Supplement (<a
href="https://www.census.gov/data/datasets/time-series/demo/cps/cps-supp_cps-repwgt/cps-voting.html">CPSVRS</a>),
and the Cooperative Election Survey (<a
href="https://cces.gov.harvard.edu">CES</a>, formerly the CCES) and a
national survey of voter preference, like the CES, to build
demographically stratified turnout and preference data at the state or
congressional-district (CD) level. We also add federal election result
data at the Congressional-district and state-wide (President and Senate)
level.</li>
<li>Use that data to <em>infer</em> a demographic model of turnout and
voter preference, with state-level effects.</li>
<li>Apply<a href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a> that model to the demographics of a
given district to generate a rough estimate of the likely election
result.</li>
</ol>
</section>
<section id="definitions-and-notation" class="level2">
<h2>Definitions and Notation</h2>
<p>Some important categories:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Name</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span
class="math inline">\(\mathcal{K}\)</span></td>
<td style="text-align: left;">A specific set of demographic groupings
(e.g., sex, education and race/ethnicity)</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span
class="math inline">\(k\)</span></td>
<td style="text-align: left;">A specific demographic grouping, i.e.,
<span class="math inline">\(\mathcal{K} = \{k\}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span
class="math inline">\(l\)</span></td>
<td style="text-align: left;">A location (E.g., a state <span
class="math inline">\(S\)</span>, or congressional district <span
class="math inline">\(d\)</span>)</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span
class="math inline">\(\mathcal{D}(S)\)</span></td>
<td style="text-align: left;">The set of congressional districts in
state <span class="math inline">\(S\)</span></td>
</tr>
</tbody>
</table>
<p>And quantities:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Name</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span
class="math inline">\(N\)</span></td>
<td style="text-align: left;">Citizen Voting Age Population (CVAP)</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span
class="math inline">\(V\)</span></td>
<td style="text-align: left;">Number of votes cast (for either a
Democrat or Republican)</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span
class="math inline">\(t\)</span></td>
<td style="text-align: left;">Turnout probability (<span
class="math inline">\(t=\textrm{Pr{Voting age citizen votes in the
election}}=V/N\)</span>)</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span
class="math inline">\(D\)</span></td>
<td style="text-align: left;">Number of votes cast for the Democratic
candidate</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span
class="math inline">\(p\)</span></td>
<td style="text-align: left;">Democratic voter preference (<span
class="math inline">\(p=\textrm{Pr{Votes for the
Democrat|Voted}}=D/V\)</span>)</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span
class="math inline">\(\rho\)</span></td>
<td style="text-align: left;">Population Density</td>
</tr>
</tbody>
</table>
<p>A couple of things are worth pointing out here:</p>
<ul>
<li><p>We've chosen to ignore third-party candidates and compute
two-party preference (the probability that a voting age citizen who
chooses either the Democrat or Republican chooses the Democrat) and
two-party share (the probability that a voter who chooses either the
Democrat or Republican chooses the Democrat). This is simpler and almost
always what we care most about.</p></li>
<li><p>We model <em>voter</em> preference not voting-age-citizen
preference. We are interested in election outcomes and those are created
by voters. But there are certainly interesting questions to address
about whether voters and non-voting adult citizens have the same
candidate preferences.</p></li>
<li><p>We will indicate the subset of each quantity in each demographic
group via subscript, as in <span class="math inline">\(N_k(g)\)</span>
is the number of voting age citizens from demographic group <span
class="math inline">\(k\)</span> in geography <span
class="math inline">\(g\)</span>.</p></li>
<li><p>The capitalized quantities can be summed directly over
demographic groupings. E.g., <span class="math inline">\(N(g)=\sum_k
N_k(g)\)</span>, <span class="math inline">\(V(g)=\sum_k
V_k(g)\)</span>, <span class="math inline">\(D(g)=\sum_k D_k(g)\)</span>
The lowercase quantities are probabilities, so they aggregate
differently.</p>
<ul>
<li><p><span class="math display">\[t(l) = V(l)/N(l) = \sum_k N_k(l)
t_k(l) / N(l) = \frac{\sum_k N_k(l) t_k(l)}{\sum_k
N_k(l)}\]</span></p></li>
<li><p><span class="math display">\[t(S) = V(S)/N(S) = \frac{\sum_{d\in
\mathcal{D}(S)}\sum_k N_k(d)t_k(d)}{N(S)}\]</span></p></li>
<li><p><span class="math display">\[p(l) = D(l)/V(l) = \frac{\sum_k
N_k(l) t_k(g) p_k(l)}{V(l)} =\frac{\sum_k N_k(l) t_k(l) p_k(l)}{\sum_k
N_k(l) t_k(l)}\]</span></p></li>
<li><p><span class="math display">\[p(S) = D(S)/V(S) = \frac{\sum_{d\in
\mathcal{D}(S)}\sum_k N_k(d) t_k(d) p_k(d)}{\sum_{d\in \mathcal{D}(S)}
\sum_k N_k(d)t_k(d)}\]</span></p></li>
</ul></li>
</ul>
<p>We <em>observe</em> <span class="math inline">\(t\)</span> and <span
class="math inline">\(p\)</span> in any location where we have survey or
election data. In the CES survey data, we observe <span
class="math inline">\(t_k(g)\)</span> and <span
class="math inline">\(p_k(g)\)</span> for demographic variables
including age, sex, education, race and ethnicity and at the geographic
level of congressional districts. In the CPSVRS survey, we observe <span
class="math inline">\(t_k(s)\)</span>, i.e., just turnout and only at
the level of states<a href="#fn4" class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a>. An election result is an
observation of <span class="math inline">\(t(s)\)</span> and <span
class="math inline">\(p(s)\)</span> or <span
class="math inline">\(t(d)\)</span> and <span
class="math inline">\(p(d)\)</span>, that is turnout and preference but
without any demographic breakdown<a href="#fn5" class="footnote-ref"
id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<p>Our goal is to model <span class="math inline">\(t\)</span> and <span
class="math inline">\(p\)</span> as functions of the location and
demographics so we can better understand them–how does race affect voter
preference in Texas?– and look for anomalies, districts where the
modeled expectation and the historical election results are very
different. Such mismatches might indicate issues in the model, but they
might also be indications of interesting political stories, and places
to look for flippable or vulnerable districts. We use <span
class="math inline">\(u\)</span> for the modeled <span
class="math inline">\(t\)</span>, and <span
class="math inline">\(q\)</span> for the modeled <span
class="math inline">\(p\)</span>.</p>
<p>Modeled Quantities</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span
class="math inline">\(u_k(l;\rho)\)</span></td>
<td style="text-align: left;">Modeled turnout probability in location
<span class="math inline">\(l\)</span> with population density <span
class="math inline">\(\rho\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span
class="math inline">\(q_k(l;\rho)\)</span></td>
<td style="text-align: left;">Modeled Democratic voter preference in
location <span class="math inline">\(l\)</span> with population density
<span class="math inline">\(\rho\)</span></td>
</tr>
</tbody>
</table>
<p>So, for a location <span class="math inline">\(l\)</span> and
demographic group <span class="math inline">\(k\)</span>, <span
class="math inline">\(u_k(l;\rho)\)</span> is the estimated probability
that a voting-age citizen in that location, with population density
<span class="math inline">\(\rho\)</span> and with those demographics,
will turn out to vote. And <span
class="math inline">\(q_k(l;\rho)\)</span> is the probability that any
such voter will choose the democratic candidate<a href="#fn6"
class="footnote-ref" id="fnref6"
role="doc-noteref"><sup>6</sup></a>.</p>
<p>Demographic Model Dependencies (collectively indexed by <span
class="math inline">\(k\)</span>)</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Sex</td>
<td style="text-align: left;">Female or Male (surveys and the ACS data
only record binary gender information)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Education</td>
<td style="text-align: left;">Non-graduate from college or college
graduate</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Race/Ethnicity</td>
<td style="text-align: left;">Black, Hispanic, Asian,
White-non-Hispanic, or Other</td>
</tr>
</tbody>
</table>
<p>The obvious missing category here is age. Our input data has age
information but when we construct demographic breakdowns of new
districts we are limited to what's provided by the ACS survey. That
information is published by the census in various tables and it's not
possible to extract age, education and race/ethnicity simultaneously<a
href="#fn7" class="footnote-ref" id="fnref7"
role="doc-noteref"><sup>7</sup></a>. So we cannot post-stratify using
the age information, even if it's present in the model. Since the model
is simpler to fit without it, we drop age information for now.</p>
</section>
<section id="basic-model-structure" class="level2">
<h2>Basic Model Structure</h2>
<p>In order to use our data to estimate turnout probability and voter
preference, we need a model of how survey or election results depend on
those quantities. The standard choices for turnout and voter preference
models is <a
href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli</a>/<a
href="https://en.wikipedia.org/wiki/Binomial_distribution">Binomial</a>–which
assumes that for each person of type <span
class="math inline">\(k\)</span> in location <span
class="math inline">\(l\)</span> with population density <span
class="math inline">\(\rho\)</span>, the choice to vote is like a coin
flip where the odds of that coin coming up "voted" is <span
class="math inline">\(u_k(l;\rho)\)</span> and coming up "didn't vote"
is <span class="math inline">\(1-u_k(l;\rho)\)</span>. Similarly, for
any voter of type <span class="math inline">\(k\)</span> in location
<span class="math inline">\(l\)</span> with population density <span
class="math inline">\(\rho\)</span>, the chance of their vote being for
the Democrat is <span class="math inline">\(q_k(l;\rho)\)</span> and the
Republican <span class="math inline">\(1-q_k(l;\rho)\)</span>. If we
modeled each voter separately, these would be <a
href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli</a>
trials and when we take groups of Bernoulli trials with the same
probability of success, we get a <a
href="https://en.wikipedia.org/wiki/Binomial_distribution">Binomial</a>
distribution.</p>
<p>The Bernoulli/Binomial is a reasonable fit for each data-set alone,
but had insufficient dispersion, even allowing for various offsets in
the parameters, to account for the different data sources all in one
model. The <a
href="https://en.wikipedia.org/wiki/Beta-binomial_distribution">Beta-Binomial</a>,
on the other hand, allows a reasonable fit to all the data. The cost of
using this slightly more complex model, is the introduction of a
parameter (one each for turnout and vote choice) which reflects the
dispersion in the observed probabilities.</p>
<p>Specifically, given <span class="math inline">\(u_k(l;\rho)\)</span>
and <span class="math inline">\(q_k(l;\rho)\)</span>, we assume that the
probability of observing <span class="math inline">\(n\)</span> voters
in a group of <span class="math inline">\(N\)</span> voting-age citizens
of type <span class="math inline">\(k\)</span> in location <span
class="math inline">\(l\)</span> with density <span
class="math inline">\(\rho\)</span> is <span
class="math inline">\(BB\big(n|N,u_k(l;\rho)\phi_T,(1-u_k(l;\rho))\phi_T\big)\)</span>
and the probability of observing <span class="math inline">\(d\)</span>
votes for the democratic candidate among <span
class="math inline">\(V\)</span> voters of type <span
class="math inline">\(k\)</span> in location <span
class="math inline">\(l\)</span> with density <span
class="math inline">\(\rho\)</span> is <span
class="math inline">\(BB\big(d|V,q_k(l;\rho)\phi_P,(1-q_k(l;\rho))\phi_P\big)\)</span>,
where the <span class="math inline">\(\phi\)</span>’s parameterize the
dispersion in probabilities.</p>
<p>We need to parameterize <span class="math inline">\(u\)</span> and
<span class="math inline">\(q\)</span> in terms of our observed
demographic variables. We use the <a
href="https://en.wikipedia.org/wiki/Logit">logit</a> function to map an
unbounded range into a probability (and thus its inverse to map back to
probability):</p>
<p><span class="math display">\[u_k(l;\rho) =
\textrm{logit}^{-1}(\alpha_l +
\vec{X}_k(\rho)\cdot\vec{\beta_l})\]</span> <span
class="math display">\[q_k(l;\rho) = \textrm{logit}^{-1}(\gamma_l +
\vec{Y}_k(\rho)\cdot\vec{\theta_l})\]</span></p>
<p>where <span class="math inline">\(\vec{X}_k(\rho)\)</span> is a
vector containing the density and demographic information. Population
density for any region is computed via a population-weighted geometric
mean: population-weighted to better reflect the lived density of the
people in the region and geometric mean because it is more robust to
outliers which are a common feature of population densities. In our
model we bin those densities into 10 quantiles and use the quantile
index in place of the actual density to further reduce the influence of
outliers. The demographic information <span
class="math inline">\(k\)</span> is encoded via "one-hot" encoding of
the category: using -1 or 1 for a binary category like college-educated
and <span class="math inline">\((M-1)\)</span> 0s or 1s for an <span
class="math inline">\(M\)</span>-valued category like
race/ethnicity.</p>
<p><span class="math inline">\(\vec{Y}_k(\rho)\)</span> is the same as
<span class="math inline">\(\vec{X}_k(\rho)\)</span> except it also
contains an element representing incumbency information for the relevant
election: a <span class="math inline">\(1\)</span> for a Democratic
incumbent, <span class="math inline">\(0\)</span> for no-incumbent and
<span class="math inline">\(-1\)</span> for a Republican incumbent.</p>
</section>
</section>
<section id="hierarchical-model" class="level1">
<h1>Hierarchical Model</h1>
<p>Rather than estimating <span
class="math inline">\(u_k(l;\rho)\)</span> and <span
class="math inline">\(q_k(l;\rho)\)</span> directly in each state from
data in only that state, we use a <a
href="https://en.wikipedia.org/wiki/Multilevel_model">multi-level
model</a>, allowing partial-pooling of the national data to inform the
estimates in each state. This introduces more parameters to the model
but allows for a more robust and informative fit. In our case, we model
the <span class="math inline">\(\alpha\)</span>, <span
class="math inline">\(\gamma\)</span>, <span
class="math inline">\(\vec{\beta}\)</span> and <span
class="math inline">\(\vec{\theta}\)</span> hierarchically. There
various ways to parameterize this and we choose a <a
href="https://betanalpha.github.io/assets/case_studies/hierarchical_modeling.html#51_Multivariate_Centered_and_Non-Centered_Parameterizations">non-centered</a>
parameterization since we expect our “contexts” (the various states), to
be relatively similar which might lead to problems fitting using a
centered paramterization.</p>
<p>So we set</p>
<p><span class="math display">\[\alpha_l = \alpha + \sigma_\alpha
a_l\]</span> <span class="math display">\[\vec{\beta}_l = \vec{\beta} +
\vec{\tau}_{\beta} \cdot \textbf{C}_{\beta} \cdot \vec{b}_l\]</span>
<span class="math display">\[\gamma_l = \gamma + \sigma_\gamma
c_l\]</span> <span class="math display">\[\vec{\theta}_l = \vec{\theta}
+ \vec{\tau}_{\theta} \cdot \textbf{C}_{\theta} \cdot
\vec{d}_l\]</span></p>
<p>where <span class="math inline">\(\sigma_\alpha\)</span>, <span
class="math inline">\(\sigma_\gamma\)</span>, <span
class="math inline">\(\vec{\tau}_\beta\)</span>, and <span
class="math inline">\(\vec{\tau}_\theta\)</span> are standard-deviation
parameters which control the amount of pooling of national data with
state data. If those parameters are small, then the fit is using mostly
unpooled, national data. If those parameters are large, then the
state-level data is mostly informing the fit. We fit those parameters
along with everything else. <span
class="math inline">\(\textbf{C}\)</span> is a correlation matrix, fit
to the correlation among the density and demographic parameters in the
data.</p>
</section>
<section id="meta-analysis-details" class="level1">
<h1>Meta-analysis details</h1>
<p>Since we are using multiple surveys and the election data–a sort of
meta-analysis–we need to account for the possibility of systematic
differences between these sources of data. Our approach to this is to
add a hierarchical (logistic) <span
class="math inline">\(\alpha\)</span> centered at 0 and varying among
the data-sets, as well as a non-hierarchical data-set-specific offsets
for each demographic category <span
class="math inline">\(\vec{\beta}\)</span> in each data-set except the
house election results<a href="#fn8" class="footnote-ref" id="fnref8"
role="doc-noteref"><sup>8</sup></a>.</p>
</section>
<section id="modeling-election-data" class="level1">
<h1>Modeling Election Data</h1>
<p>So far we’ve dodged one subtlety. Unlike the survey data, the
election data does not come with attached demographic information. One
approach is to remove it from the multi-level model and then, after
fitting the multi-level model to the survey data, adjusting the
model-parameters such that post-stratification matches some set of
aggregates from the election data, for example, turnout in each state.
This is done by choosing a “smallest” such adjustment in some
well-specified sense. See, e.g., <a
href="http://www.stat.columbia.edu/~gelman/research/published/misterp.pdf">“Deep
Interactions With MRP...</a>, pp. 769-770.</p>
<p>As discussed in that paper, such an adjustment assumes small
correlations between whatever leads to mismatches between the survey and
the election results and the demographic variables in question. It’s not
clear why that should be the case. In the case of the CPS at least,
there is <a
href="https://static1.squarespace.com/static/5fac72852ca67743c720d6a1/t/5ff8a986c87fc6090567c6d0/1610131850413/CPS_AFS_2021.pdf">evidence</a>
that this assumption doesn’t hold.</p>
<p>And there is also a technical issue: When adjusting the parameters
this way it is unclear what becomes of their distributions. Depending on
how we do the estimation (see next section), we may have more
information about each parameter than its likeliest value, including
some information about uncertainty or a confidence interval. But it’s
not at all clear how that information should be adjusted via the
procedure outlined in the paper above.</p>
<p>There is, however, an alternative. We can add the likelihood of the
parameters explaining the election data to the model itself. We do this
by constructing new parameters for each election <span
class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[\hat{u}^{(i)} = \frac{\sum_k N^{(i)}_k
u_k(l^{(i)};\rho^{(i)})}{N^{(i)}}\]</span> <span
class="math display">\[\hat{q}^{(i)} = \frac{\sum_k N^{(i)}_k
u_k(l^{(i)};\rho^{(i)}) q_k(l^{(i)},\rho^{(i)})}{\sum_k N^{(i)}_k
u_k(l^{(i)};\rho^{(i)})}\]</span></p>
<p>and then asserting that the election is governed by the same
beta-binomial process as the surveys. That is given parameters <span
class="math inline">\(u\)</span> and <span
class="math inline">\(q\)</span>, the probability that, in election
<span class="math inline">\(i\)</span>, given <span
class="math inline">\(N\)</span> voting-age citizens in the election
location <span class="math inline">\(l^{(i)}\)</span>, the probability
that there were <span class="math inline">\(V^{(i)}\)</span> voters,
<span class="math inline">\(D^{(i)}\)</span> of whom voted for the
Democratic candidate are <span
class="math inline">\(BB\big(V^{(i)}|N^{(i)},\hat{u}^{(i)}\phi_T,(1-\hat{u}^{(i)})\phi_T\big)\)</span>
and <span
class="math inline">\(BB\big(D^{(i)}|V^{(i)},\hat{q}^{(i)}\phi_P,(1-\hat{q}^{(i)})\phi_P\big)\)</span>
respectively.</p>
<section id="practical-considerations" class="level2">
<h2>Practical Considerations</h2>
<p>At this point, we are, theoretically, done. We have parameterized our
probabilities via our data, and asserted that each piece of data, survey
or election, has a likelihood expressed by a Beta-Binomial distribution
using those parameters. So we can construct the joint likelihood and
attempt to maximize it in order to estimate our parameters. Or we can
use a Bayesian approach, choose priors for our various parameters,
combine those with the probability of the data given the parameters (the
likelihood) and use that to find the joint posterior distribution of our
parameters given our data, etc.</p>
<p>Let’s recall that our goal is to apply these estimations to new
geographical regions via post-stratification. So, optimally, we’d like
some way of computing a distribution of post-stratification results that
is consistent with the distribution of parameter values, since the
uncertainties we are ultimately interested in are those of the
post-stratifications.</p>
<p>Monte-Carlo simulation is ideal here, since it allows computation of
derived quantities on each path, resulting in a distribution of
post-stratified results. Further, Monte-Carlo simulation does not
require that we analytically maximize this very complex likelihood,
though, depending on the method, it does require gradients of the
likelihood.</p>
<p>If we only want to model the surveys, this is straightforward and
fast. However, once we include the election results, things get harder
since the <span class="math inline">\(\hat{u}\)</span> and <span
class="math inline">\(\hat{q}\)</span>, being sums over many <span
class="math inline">\(\textrm{logit}^{-1}\)</span> functions, make the
gradients of the likelihood computationally burdensome. In practice,
these models run in a few hours on a reasonably powerful laptop.</p>
<p>We use <a href="https://www.haskell.org">Haskell</a> to parse the
data, reorganize it and produce code for <a
href="https://mc-stan.org">Stan</a>, which then runs the <a
href="https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo">Hamiltonian
Monte Carlo</a> to estimate the posterior distribution of all the
parameters and thus posterior distributions of <span
class="math inline">\(u_k(l,\rho)\)</span> and <span
class="math inline">\(q_k(l,\rho)\)</span>.</p>
<p>To produce demographic profiles of new districts, we get shapefiles
from <a href="https://davesredistricting.org">Dave’s Redistricting</a>,
ACS data from the Census Bureau via <a
href="https://www.nhgis.org">NHGIS</a> and use areal interpolation<a
href="#fn9" class="footnote-ref" id="fnref9"
role="doc-noteref"><sup>9</sup></a> to compute the overlap of each
census geography (tracts<a href="#fn10" class="footnote-ref"
id="fnref10" role="doc-noteref"><sup>10</sup></a>, in this case) with
our new districts and then aggregate that census information for the
district.</p>
<p>Using those distributions and demographic breakdowns (which amounts
to the numbers, <span class="math inline">\(N_k(l)\)</span> of eligible
voters for each combination of sex, education-level and race), we <a
href="https://en.wikipedia.org/wiki/Multilevel_regression_with_poststratification">post-stratify</a>
to get from our parameter values to votes <span
class="math inline">\(V\)</span>, and democratic votes, <span
class="math inline">\(D\)</span>:</p>
<p><span class="math display">\[V(d) = \sum_k N_k(d)
u_k(l,\rho)\]</span> <span class="math display">\[D(d) = \sum_k N_k(l)
u_k(l,\rho) q_k(l,\rho)\]</span></p>
<p>One advantage of estimating these things via Monte Carlo, is that we
compute each quantity, including the post-stratified ones, on
<em>each</em> Monte Carlo iteration. So rather than just getting an
estimate of the value of each quantity, we get some large number of
samples from its posterior distribution. So we have good diagnostics
around these quantities: we can see if we’ve done enough sampling for
their distributions to have converged and we can extract informative
confidence intervals–even of derived quantities like the
post-stratifications–rather than just crude estimates of standard
deviation as you might get from various methods which estimate
parameters via optimization of the maximum-likelihood function.</p>
<p><em>Want to read more from Blue Ripple? Visit our <a
href="https://www.blueripplepolitics.org"><strong>website</strong></a>,
sign up for <a href="http://eepurl.com/gzmeQ5"><strong>email
updates</strong></a>, and follow us on <a
href="https://twitter.com/BlueRipplePol"><strong>Twitter</strong></a>
and <a
href="https://www.facebook.com/blueripplepolitics"><strong>FaceBook</strong></a>.
Folks interested in our data and modeling efforts should also check out
our <a href="https://github.com/blueripple"><strong>Github</strong></a>
page.</em></p>
</section>
</section>
<section class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>By “overlap” we mean that we weight a
census geography in a district by computing how much the area of that
geography overlaps the district, a technique called <a
href="https://www.spatialanalysisonline.com/HTML/areal_interpolation.htm">“areal
interpolation”</a>. Areal interpolation assumes that people are
distributed evenly over the source geographies (the census blocks or
block-groups). This assumption is relatively harmless if the census
geographies are very small compared to the target geography. More
accuracy can be achieved using sources of population density data within
the census geographies, for example, the <a
href="https://www.usgs.gov/centers/eros/science/national-land-cover-database?qt-science_center_objects=0#qt-science_center_objects">National
Land Cover Database (NLCD)</a>.<a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>For the decennial census there is
detailed data available at the block level, where a block has ~100
people. For the American Community Survey, which collects data every
year and thus is more up to date than the decennial census, data is
available only at the “block-group” level, consisting of a few thousand
people.<a href="#fnref2" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Given a model giving a probability
that any person, described by their demography, will vote and the
probability that they will vote for the Democratic candidate, we can
break each SLD into buckets of each sort of person in our model and then
multiply the number of people in each group by the those probabilities
to figure out the total number of votes in each group and how many of
those votes were for the Democratic candidate. We add all those numbers
up and that yields turnout and Dem vote share. Applying the model to the
composition of the SLD in this way is called “post-stratification”.<a
href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>The CPSVRS is reported with
county-level geography. Counties can be smaller or larger than a
congressional district and in practice, these geographies are hard to
convert. Rather than deal with that, we aggregate the CPSVRS to the
state-level.<a href="#fnref4" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>It's possible, via exit polls or the
voter file (a public record in each state matching voters to names and
addresses) to try and estimate the demographics of actual election
turnout. In practice, these methods are unreliable (exit polls) or
expensive and hard to get right (purchasing, cleaning and matching
voter-file data).<a href="#fnref5" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>The explicit presence of the
population density <span class="math inline">\(\rho\)</span> here is
potentially confusing. It might seem like population density should
simply be part of what varies with the location, <span
class="math inline">\(l\)</span>. In our particular setup, we use
population density data at a level more granular than our modeled set of
locations. Most of our data is state-level. And what data we have at the
congressional district level (CES survey and federal house elections) is
for the districts which existed from the 2012-2020 elections. Many of
those districts have now changed. And we are interested in
state-legislative districts as well. So we use states as modeling
locations rather than congressional districts. But, like demographic
variations, population density is known to be strongly predictive of
voter preference–people in cities are much more likely to vote for
Democrats than people in suburbs or rural areas and this effect persists
even when accounting for differences in age and race between those
groups. It’s relatively easy to gather population density information
for any geography of interest: states, house districts and
state-legislative districts. We handle the density differently from the
other demographic variables because density is a continuous or ordinal
predictor rather than categorical, like sex, education or race.<a
href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>Some ACS tables do have age
information, but then we lose education or race/ethnicity. There are
interesting techniques using those tables together to extract
probability distributions of age given sex, education and
race/ethnicity. We plan to apply these techniques at some point to add
age into the model.<a href="#fnref7" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p>There are two things to explain here.
Why isn’t the data-set <span class="math inline">\(\beta\)</span> also
hierarchical? And why not have a <span
class="math inline">\(\vec{\beta}\)</span> offset for all data-sets? We
tried a hierarchical data-set-<span class="math inline">\(\beta\)</span>
but it was difficult to fit and, we think, requires a mix of
centered/non-centered parameterizations. So we switched to a
non-hierarchical set of offsets for <span
class="math inline">\(\vec{\beta}\)</span>. But once the parameter is
non-hierarchical, having an offset for each data-set would introduce a
redundancy (technically, the parameters would be “<a
href="https://en.wikipedia.org/wiki/Identifiability">non-identifiable</a>”)
because a fixed amount could be added to each data-set <span
class="math inline">\(\vec{\beta}\)</span> and removed from the shared
<span class="math inline">\(\vec{\beta}\)</span> without changing the
estimated probabilities. To fix that, we pick one data-set as “neutral”
and don’t add the demographic offset there. Since we are interested in
house and state-legislative elections, using the house election as the
“neutral” data-set makes the most sense.<a href="#fnref8"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>By “overlap” we mean that we weight a
census geography in a district by computing how much the area of that
geography overlaps the district, a technique called <a
href="https://www.spatialanalysisonline.com/HTML/areal_interpolation.htm">“areal
interpolation”</a>. Areal interpolation assumes that people are
distributed evenly over the source geographies (the census blocks or
block-groups). This assumption is relatively harmless if the census
geographies are very small compared to the target geography. More
accuracy can be achieved using sources of population density data within
the census geographies, for example, the <a
href="https://www.usgs.gov/centers/eros/science/national-land-cover-database?qt-science_center_objects=0#qt-science_center_objects">National
Land Cover Database (NLCD)</a>.<a href="#fnref9" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p>For the decennial census there is
detailed data available at the block level, where a block has ~100
people. For the American Community Survey, which collects data every
year and thus is more up to date than the decennial census, data is
available only at the “block-group” level, consisting of a few thousand
people.<a href="#fnref10" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<!--
  <div class="br-readmore">
  Want To read more from Blue Ripple? Visit our
  <a href="https://blueripple.org">website</a>,
  sign up for <a href="http://eepurl.com/gzmeQ5">email updates</a>,
    and follow us on <a href="https://twitter.com/BlueRipplePol">Twitter</a>
    and <a href="https://www.facebook.com/blueripplepolitics/">FaceBook</a>.
    Folks interested in our data and modeling efforts should also check out
    our <a href="https://github.com/blueripple">Github page</a>.
  </div>
-->
<!-- page content ends here -->

</div>     <!-- pure-u-1... -->
</div>     <!-- pure-g -->
</section> <!-- page-content -->
<footer>
  <div class="br-header">
    <div class="br-header-item br-header-br" style="width: 8em">
      <p>BLUE RIPPLE POLITICS</p>
    </div>
    <div class="br-header-item br-home-button-c">
      <a href="https://www.blueripplepolitics.org">
	<div class="br-home-button-b">HOME</div>
      </a>
    </div>
  </div>
</footer>
<!--
<script>
var mindoc=function(){function e(e){return e=e.toLowerCase(),e.charAt(0).toUpperCase()+e.substr(1)}function n(e){var n=new RegExp(/^\b[a-z]\S+\b-\b\S+\b/);return n.test(e)&&(e=e.replace(/-+/g," ")),e}function t(t){return t=n(t),e(t)}function r(e,n){return!!e.className.match(new RegExp("(\\s|^)"+n+"(\\s|)"))}function a(e,n){r(e,n)||(e.className+=" "+n)}function o(e,n){if(r(e,n)){var t=new RegExp("(\\s|^)"+n+"(\\s|)");e.className=e.className.replace(t," ")}}function u(){var e,n,t={table:"pure-table pure-table-bordered"};Object.keys(t).forEach(function(r){if(e=document.getElementsByTagName(r),n=e.length,n>1)for(var o=0;n>o;o++)a(e[o],t[r])})}function c(e,n){for(var t=0,r=e.length;r>t;t++)for(var a=e[t].getElementsByTagName("a"),u=0,c=a.length;c>u;u++)a[u].addEventListener("click",function(){o(n,"hidden")})}function i(e,n){for(var t=0,r=e.length;r>t;t++)e[t].addEventListener("click",function(){o(n,"hidden")})}function d(){var e=document.createElement("li");return a(e,"pure-menu-item"),e}function l(e){var n=document.createElement("a");return n.id="menu-"+e,n.href="#",n.innerHTML=t(e),a(n,"pure-menu-link"),n}function m(e){var n=document.createDocumentFragment(),t=document.createElement("nav"),r=document.createElement("div"),o=document.createElement("ul");n.appendChild(t),t.appendChild(r),r.appendChild(o),a(r,"pure-menu"),a(o,"pure-menu-list");var u="All sections",c=d();a(c,"pure-menu-selected"),o.appendChild(c),c.appendChild(l(u));for(var i,m,s=0,f=e.length;f>s;s++)i=e[s].getAttribute("id"),m=d(),o.appendChild(m),m.appendChild(l(i));var p=document.getElementById("page-content");document.querySelector("body").insertBefore(n,p)}function s(e){var n;e.hasAttribute("pure-menu-selected")||(n=document.querySelector(".pure-menu-selected"),o(n,"pure-menu-selected"),a(e,"pure-menu-selected"))}function f(e,n){var t,u=n.getAttribute("id"),c=u.replace(/menu-/,""),i=document.getElementById(u).parentNode;s(i);for(var d in e)t=e[d],r(t,"hidden")||a(t,"hidden"),t.getAttribute("id")===c&&r(t,"hidden")&&o(t,"hidden")}function p(e){var n;for(var t in e)n=e[t],r(n,"hidden")&&o(n,"hidden")}function v(e){for(var n=document.querySelectorAll(".pure-menu-link"),t=0,r=n.length;r>t;t++)0===t?n[t].addEventListener("click",function(){p(e)}):n[t].addEventListener("click",function(){f(e,this)})}return{main:function(){if(u(),document.getElementsByClassName("level2").length>0){var e,n=[];["abstract","level2","footnotes"].forEach(function(t){e=document.getElementsByClassName(t);for(var r=0,a=e.length;a>r;r++)n.push(e[r])});var t;for(var o in n)t=n[o],r(t,"level2")||a(t,"level2"),r(t,"footnotes")&&t.setAttribute("id","footnotes");m(n),v(n);var d=document.getElementsByClassName("citation"),l=document.getElementById("references");c(d,l);var s=document.getElementsByClassName("footnoteRef"),f=document.getElementById("footnotes");i(s,f)}}}}();window.addEventListener("load",function(){mindoc.main()});
</script>
-->
<!-- For debugging local scripts -->
<!-- <script src="../build/mindoc.js"></script> -->
</body>
</html>
